6.5840 2025 Lecture 1: Introduction MIT Distributed Systems Engineering 
MIT 6.824 分散式系統課程：講義 1 概述

What I mean by "distributed system":　a group of computers cooperating to provide a service
所謂的「分散式系統」是什麼意思：一組電腦合作提供服務

Examples
we all use distributed systems, popular apps' back-ends, e.g. for messaging、big web sites、 phone system
我們都使用分散式系統，流行應用程序的後端，例如用於消息傳遞、大型網站、電話系統
focus here is distributed infrastructure: 這裡的重點是分散式基礎設施
  - storage 存儲
  - transaction systems 交易系統
  - "big data" processing frameworks “大數據”、處理框架
  - authentication services 身份驗證服務

It's not easy to build systems this way: 這樣建系統並不容易
  - concurrency 並發性
  - complex interactions 複雜的交互作用
  - performance bottlenecks 效能瓶頸
  - partial failure 部分故障

So why do people build distributed systems? 那麼為什麼要建立分散式系統呢？
  - to increase capacity via parallel processing 通過並行處理增加處理量
  - to tolerate faults via replication　通過複製容許故障(容錯)
  - to match distribution of physical devices e.g. sensors 匹配實體設備（例如感測器）的分佈
  - to increase security via isolation 通過隔離提高安全性

Why study this topic? 為什麼要研讀這個主題？
  - interesting -- hard problems, powerful solutions 有趣——難題，強大的解決方案
  - widely used -- driven by the rise of big Web sites 廣泛使用——由大型網站的興起推動
  - active research area -- important unsolved problems 活躍的研究領域——未解決的重要問題
  - challenging to build -- you'll do it in the labs 構建具有挑戰性——您將在實驗中完成

COURSE STRUCTURE  課程結構
http://pdos.csail.mit.edu/6.5840
Course staff: 課程工作人員
  Frans Kaashoek and Robert Morris, lecturers 講師
  Kenneth Choi, TA
  Yun-Sheng Chang, TA
  Ivy Wu, TA
  Aryan Kumar, TA
Course components : lectures, papers, two exams, labs, final project (optional)
課程組成部分：講座、試卷、兩次考試、實驗、期末專案（可選）

Lectures : big ideas, paper discussion, lab guidance
講座：核心概念、論文討論、實驗指導

Papers: 論文
  one per lecture
  research papers, some classic, some new 
  problems, ideas, implementation details, evaluation
  please read papers before class!
  web site has a short question for you to answer about each paper 
  and we ask you to send us a question you have about the paper
  submit answer and question before start of lecture
論文要求：
  - 每堂課一篇論文
  - 每場講座對應一篇研究論文，包含經典和新近論文
  - 論文討論問題、想法、實現細節和評估
  - 課前閱讀：請在課前閱讀指定論文！
  - 網站作業：
    網站上針對每篇論文提供一個必需回答簡短的問題
    請提交一個對該論文的問題
  - 在講座開始前提交答案和問題

Exams:
  - Mid-term exam in class
  - Final exam during finals week
  - Mostly about papers and labs
  - You must attend the exams!
考試:
  - 課堂期中考
  - 在最後一周期末考試
  - 主要關於論文和實驗
  - 你必須參加考試！

Labs:
  - goal: use and implement some important techniques
  - goal: experience with distributed programming
  - first lab is due a week from Friday
  - one per week after that for a while
關於實驗：
  - 使用和實施一些重要技術
  - 分散式程式設計經驗
  - 第一個實驗從星期五開始一周到期
  - 之後每週一次，持續一段時間

Lab 1: distributed big-data framework (like MapReduce)
Lab 2: client/server vs unreliable network
Lab 3: fault tolerance using replication (Raft)
Lab 4: a fault-tolerant database
Lab 5: scalable database performance via sharding
We grade the labs using a set of tests, we give you all the tests; none are secret
實驗作業：
  - 實驗 1：分散式大數據框架（類似 MapReduce）
  - 實驗 2：客戶端/伺服器 vs. 不可靠網路
  - 實驗 3：使用複製實現容錯（Raft 算法）
  - 實驗 4：容錯資料庫
  - 實驗 5：透過分片提升資料庫可擴展性效能
  - 評分方式：我們使用一組測試來評分實驗，所有測試均公開，沒有隱藏測試

Optional final project at the end, in groups of 2 or 3. 
  The final project substitutes for Lab 5.
  You think of a project and clear it with us.
  Code, short write-up, demo on last day. 
期末專案（選修）：
  - 以 2 或 3 人為一組進行
  - 期末專案可替代實驗 5
  - 需自行構思專案並與我們確認
  - 需提交程式碼、簡短報告，並在最後一天進行展示

Warning:
  - debugging the labs can be time-consuming
  - start early
  - ask questions on Piazza
  - use the TA office hours
注意事項：
  - 實驗作業除錯可能耗時
  - 提早開始
  - 在 Piazza 上提問
  - 善用助教（TA Teaching Assistant）辦公時間

MAIN TOPICS  主要主題
This is a course about infrastructure for applications.本課程聚焦於應用程式的基礎設施
  - Storage.儲存
  - Communication.通訊
  - Computation.運算

A big goal: hide the complexity of distribution from applications.
核心目標：對應用程式隱藏分散式系統的複雜性

Topic: fault tolerance
  1000s of servers, big network -> always something broken 
    We'd like to hide these failures from the application
    "High availability": service continues despite failures 
  Big idea: replicated servers. 
    If one server crashes, can proceed using the other(s).
主題：容錯
  在數千台伺服器和大型網絡中，總有部分元件會故障
    - 目標：對應用程式隱藏這些故障
    - 高可用性：即使發生故障，服務仍能繼續運行
  核心概念：複製伺服器
    - 若一台伺服器當機，可使用其他伺服器繼續運作

Topic: consistency
  General-purpose infrastructure needs well-defined behavior.　
    E.g. "read(x) yields the value from the most recent write(x)."
  Achieving good behavior is hard!
    e.g. "replica" servers are hard to keep identical.
主題：一致性
  通用基礎設施需要明確定義的行為：例如：「read(x) 應返回最近一次 write(x) 的值。」
  實現良好行為的挑戰：例如：保持「複製」伺服器完全一致非常困難

Topic: performance
  The goal: scalable throughput　Nx servers -> Nx total throughput via parallel CPU, RAM, disk, net.
  Scaling gets harder as N grows:　
    Load imbalance.
    Slowest-of-N latency.
主題：效能
  目標：可擴展的吞吐量：N 台伺服器應實現 N 倍總吞吐量，透過並行使用 CPU、記憶體、磁碟和網路
  隨著 N 增加，擴展難度加大：
    - 負載不平衡
    - N 台伺服器中最慢的延遲（slowest-of-N latency）

Topic: tradeoffs
  Fault-tolerance, consistency, and performance are enemies.
  Fault tolerance and consistency require communication
    e.g., send data to backup server
    e.g., check if cached data is up-to-date
    communication is often slow and non-scalable
  Many designs sacrifice consistency to gain speed.
    e.g. read(x) might *not* yield the latest write(x)!
    Painful for application programmers (or users).
  We'll see many design points in the consistency/performance spectrum.
主題：權衡
  . 容錯、一致性和效能相互衝突：
    容錯和一致性需要通訊：
      - 例如：將數據傳送到備份伺服器。
      - 例如：檢查快取數據是否為最新。
      - 通訊通常緩慢且不具可擴展性。
  . 許多設計犧牲一致性以換取速度：
    - 例如：read(x) 可能無法返回最新的 write(x)！
    - 這對應用程式開發者（或用戶）來說很麻煩。
  . 我們將探討一致性與效能光譜中的多種設計點。

Topic: implementation
  RPC, threads, concurrency control, configuration.
  The labs...
主題：實行細節
  - 技術議題：遠端程序呼叫（RPC）、執行緒、並行控制、配置管理
  - 實驗作業：課程中的實驗將涉及這些技術

This material comes up a lot in the real world.
  All big web sites and cloud providers are expert at distributed systems.
  Many open source projects are built around these ideas.
  A hot topic in both academia and industry.
分散式系統技術廣泛應用於現實世界：
  - 所有大型網站和雲端服務提供者都是分散式系統專家
  - 許多開源專案圍繞這些概念構建
  - 在學術界和產業界都是熱門話題

CASE STUDY: MapReduce
Let's talk about MapReduce (MR)
  a good illustration of 6.5840's main topics
  hugely influential
  the focus of Lab 1
案例研究：MapReduce
介紹 MapReduce
  - MapReduce 是本課程（6.5840）主要主題的絕佳範例
  - 極具影響力
  - 實驗 1 的焦點

MapReduce overview
  context: multi-hour computations on multi-terabyte data-sets
    e.g. build search index, or sort, or analyze structure of web
    only practical with 1000s of computers
  big goal: easy for non-specialist programmers
    programmer just defines Map and Reduce functions　
    often fairly simple sequential code
  MR manages, and hides, all aspects of distribution!
MapReduce 概覽
  . 背景：處理數小時、數 TB 數據的計算任務
    - 例如：建立搜尋索引、排序或分析網頁結構
    - 僅在數千台電腦上才實際可行
  . 核心目標：讓非專業程式設計者也能輕鬆使用
    - 程式設計者只需定義 Map 和 Reduce 函數
    - 這些函數通常是簡單的順序程式碼
  . 管理並隱藏所有分散式細節

Abstract view of a MapReduce job -- word count
  Input1 -> Map -> a,1 b,1
  Input2 -> Map ->     b,1
  Input3 -> Map -> a,1     c,1
                    |   |   |
                    |   |   -> Reduce -> c,1
                    |   -----> Reduce -> b,2
                    ---------> Reduce -> a,2
1) input is (already) split into M pieces
2) MR calls Map() for each input split, produces list of k,v pairs "intermediate" data. each Map() call is a "task"
3) when Maps are done, MR gathers all intermediate v's for each k,and passes each key + values to a Reduce call
4) final output is set of <k,v> pairs from Reduce()s
MR工作的抽象視圖——字數統計
1）輸入數據（已）分成 M 個分片
2）MR為每個輸入分片呼叫 Map()，產生鍵值對（k,v）的「中間數據」, 每個 Map() 呼叫是一個「任務」 
3）當所有 Map 完成後，MapReduce 收集每個鍵（k）的所有中間值（v），並將鍵值對傳遞給 Reduce 呼叫
4）最終輸出是 Reduce() 產生的 <k,v> 鍵值對集合

Word-count code 字數統計程式碼
  Map(d)
    chop d into words //將 d 切成單詞
    for each word w   //對每個單詞 w
      emit(w, "1")    //輸出(w, "1")
  Reduce(k, v[])
    emit(len(v[]))    //輸出(len(v[]))

MapReduce scales well:
  N "worker" computers (might) get you Nx throughput. 
    Maps()s can run in parallel, since they don't interact.
    Same for Reduce()s.
  Thus more computers -> more throughput -- very nice!
MR的可擴展性
  . N 台「工作節點」電腦（可能）帶來 N 倍吞吐量
    - Map() 任務可並行執行，因彼此不相互影響
    - Reduce() 任務亦同
  . 因此，更多電腦 -> 更高吞吐量，非常理想！

MapReduce hides much complexity:
  sending map+reduce code to servers
  tracking which tasks have finished
  "shuffling" intermediate data from Maps to Reduces
  balancing load over servers
  recovering from crashed servers
MR隱藏的複雜性
  - 將 Map 和 Reduce 程式碼發送到伺服器
  - 追蹤任務完成情況
  - 將中間數據從 Map 傳送到 Reduce（稱為「洗牌」）
  - 在伺服器間平衡負載
  - 從故障伺服器中恢復

To get these benefits, MapReduce restricts applications:
  No interaction or state (other than via intermediate output).
  Just the one Map/Reduce pattern for data flow.
  No real-time or streaming processing.
MR的應用限制
  - 無交互或狀態（僅透過中間輸出）
  - 僅支援單一 Map/Reduce 數據流模式
  - 不支援即時或串流處理

Some details (paper's Figure 1)
Input and output are stored on the GFS cluster file system 
  MR needs huge parallel input and output throughput. 
  GFS splits files over many servers, many disks, in 64 MB chunks
    Maps read in parallel. Map
    Reduces write in parallel
  GFS replicates all data on 2 or 3 servers, for fault tolerance
  GFS is a big win for MapReduce
The "Coordinator" manages all the steps in a job.
  1. coordinator gives Map tasks to workers until all Maps complete
     Maps write output (intermediate data) to local disk
     Maps split output, by hash(key) mod R, into one file per Reduce task
  2. after all Maps have finished, coordinator hands out Reduce tasks
     each Reduce task corresponds to one hash bucket of intermediate output
     each Reduce task fetches its bucket from every Map worker
     sort by key, call Reduce() for each key
     each Reduce task writes a separate output file on GFS
細節（參考論文圖 1）
  . 輸入與輸出儲存於 GFS 叢集檔案系統：
    . MR需要極高的並行輸入/輸出吞吐量
    . GFS 將檔案分割到多台伺服器、多個磁碟，塊大小為 64 MB
      - 並行讀取數據
      - Reduce 並行寫入數據
    . GFS 將所有數據複製到 2 或 3 台伺服器，實現容錯
    . GFS 對 MapReduce 的效能至關重要

  . 協調者（Coordinator）管理作業的所有步驟：
    1.協調者分配 Map 任務給工作節點，直到所有 Map 完成。
      - Map 將中間數據寫入本地磁碟。
      - Map 根據 hash(key) mod R，將輸出分割為每個 Reduce 任務的檔案。
    2.所有 Map 完成後，協調者分配 Reduce 任務。
      - 每個 Reduce 任務對應一個中間輸出的哈希桶。
      - 每個 Reduce 任務從所有 Map 工作節點獲取其對應桶。
      - 按鍵排序，針對每個鍵呼叫 Reduce()。
      - 每個 Reduce 任務將結果寫入 GFS 的獨立輸出檔案

What will likely limit the performance?
  We care since that limit is the thing to optimize.
  CPU? memory? disk? network?
  In 2004 authors were limited by network speed.
    What does MR send over the network?
      Maps read input from GFS.
      Reduces fetch Map intermediate output -- the shuffle.
        Often as large as input, e.g. for sorting.
      Reduces write output files to GFS.
    [diagram: servers, tree of network switches]
    In MR's all-to-all shuffle, half of traffic goes through root switch.
    Paper's root switch: 100 to 200 gigabits/second, total
      1800 machines, so 55 megabits/second/machine.
      55 is small: less than disk or RAM speed.
效能瓶頸
  關鍵問題：什麼限制了效能？我們需要最佳化瓶頸。CPU、記憶體、磁碟、網路？
  2004 年論文指出受限於網路速度：
    MapReduce 的網路傳輸：
      . Map 從 GFS 讀取輸入。
      . Reduce 獲取 Map 的中間輸出（洗牌過程）。
        - 中間數據量常與輸入數據量相當（例如排序）。
      . Reduce 將輸出寫入 GFS。
    網路架構：[伺服器與網路交換器樹狀結構圖]
      . 在 MapReduce 的全對全洗牌中，一半流量通過根交換器。
      . 論文中的根交換器：總計 100-200 Gbit/s。
        - 1800 台機器，平均每台 55 Mbit/s。
        - 55 Mbit/s 相對較低，遠低於磁碟或記憶體速度

How does MR minimize network use?
  Coordinator tries to run each Map task on GFS server that stores its input.
    All computers run both GFS and MR workers
    So Map input is usually read from GFS data on local disk, not over network.
  Intermediate data goes over network just once.
    Map worker writes to local disk.
    Reduce workers read from Map worker disks over the network.
    (Storing it in GFS would require at least two trips over the network.)
  Intermediate data hash-partitioned into files holding many keys.
    Reduce task unit is a hash bucket (rather than just one key).
    Big network transfers are more efficient.
MapReduce 如何減少網路使用
  . 協調者嘗試將 Map 任務分配到儲存輸入數據的 GFS 伺服器。
    - 所有電腦同時運行 GFS 和 MapReduce 工作節點。
    - 因此，Map 輸入通常從本地 GFS 磁碟讀取，而非透過網路。
  . 中間數據僅透過網路傳輸一次：
    - Map 工作節點寫入本地磁碟。
    - Reduce 工作節點從 Map 工作節點磁碟透過網路讀取。
    -（若儲存在 GFS，則需至少兩次網路傳輸。）
  . 中間數據按哈希分割成包含多個鍵的檔案：
    - Reduce 任務單位是一個哈希桶（而非單一鍵）。
    - 大型網路傳輸更有效率。

How does MR get good load balance?
  Why do we care about load balance?
    If one server has more work than others, or is slower,
    then other servers will lie idle (wasted) at the end, waiting.
  But tasks vary in size, and computers vary in speed.
  Solution: many more tasks than worker machines.
    Coordinator hands out new tasks to workers who finish previous tasks.
    So faster servers do more tasks than slower ones.
    And slow servers are given less work, reducing impact on total time.
MapReduce 如何實現負載平衡
  . 為什麼重視負載平衡？
    - 若一台伺服器負載過重或速度較慢，其他伺服器將閒置（浪費資源），等待完成。
  . 挑戰：任務大小不一，電腦速度不同。
  . 解決方案：任務數遠多於工作節點數。
    - 協調者將新任務分配給完成先前任務的工作節點。
    - 較快的伺服器執行更多任務，較慢的伺服器負擔較少，減少對總時間的影響。

What about fault tolerance?
  What if a worker computer crashes?
  We want to hide failures from the application programmer!
  Does MR have to re-run the whole job from the beginning?
    Why not?
  MR re-runs just the failed Map()s and Reduce()s.
Suppose MR runs a Map task twice, one Reduce sees first run's output,
    but another Reduce sees the second run's output?
  The two Map executions had better produce identical intermediate output!
  Map and Reduce should be pure deterministic functions:
    they are only allowed to look at their arguments/input.
    no state, no file I/O, no interaction, no external communication,
      no random numbers.
  Programmer is responsible for ensuring this determinism.
容錯機制
  . 問題：若工作節點當機，MapReduce 如何處理？
  . 目標：對應用程式程式設計者隱藏故障！
  . 是否需要從頭重新運行整個作業？
    - 不需要！MapReduce 僅重新運行失敗的 Map() 或 Reduce() 任務。
  . 問題：若 Map 任務執行兩次，一個 Reduce 看到第一次的輸出，另一個看到第二次的輸出？
    - 兩次 Map 執行必須產生完全相同的中間輸出！
    - Map 和 Reduce 應為純粹確定性函數：
      . 僅能存取其參數/輸入。
      . 無狀態、無檔案 I/O、無交互、無外部通訊、無隨機數。
    - 程式設計者需負責確保這種確定性。

Details of worker crash recovery:
  * a worker crashes while running a Map task:
    coordinator notices worker is not responding
    coordinator knows which Map tasks ran on that worker
      those tasks' intermediate output is now lost, must be re-created
      coordinator tells other workers to run those tasks
    can omit re-running if all Reduces have fetched the intermediate data
  * a worker crashes while running a Reduce task:
    finished tasks are OK -- stored in GFS, with replicas.
    coordinator re-starts worker's unfinished tasks on other workers.
工作節點當機恢復細節：
  * Map 任務當機：
    . 協調者注意到工作節點無回應。
    . 協調者知道哪些 Map 任務在該節點上運行。
      - 這些任務的中間輸出丟失，需重新生成。
      - 協調者通知其他工作節點重新執行這些任務。
      - 若所有 Reduce 已獲取中間數據，可省略重新運行。
  * Reduce 任務當機：
    . 已完成的任務無影響，輸出儲存在 GFS（有複本）。
    . 協調者將未完成的任務重新分配給其他工作節點

Other failures/problems:
  * What if the coordinator gives two workers the same Map() task?
    perhaps the coordinator incorrectly thinks one worker died.
    it will tell Reduce workers about only one of them.
  * What if the coordinator gives two workers the same Reduce() task?
    they will both try to write the same output file on GFS!
    atomic GFS rename prevents mixing; one complete file will be visible.
  * What if a single worker is very slow -- a "straggler"?
    perhaps due to flakey hardware.
    coordinator starts a second copy of last few tasks.
  * What if a worker computes incorrect output, due to broken h/w or s/w?
    too bad! MR assumes "fail-stop" CPUs and software.
  * What if the coordinator crashes?
其他故障/問題：
  * 若協調者將同一 Map 任務分配給兩個工作節點？
    . 可能因協調者誤認為某工作節點已當機。
    . 協調者僅通知 Reduce 工作節點使用其中一個的輸出。
  * 若協調者將同一 Reduce 任務分配給兩個工作節點？
    . 兩者可能同時寫入同一 GFS 輸出檔案！
    . GFS 的原子性重新命名機制防止混淆，僅顯示一個完整檔案。
  * 若單一工作節點非常慢（「落後者」）？
    . 可能因硬體或軟體問題。
    . 協調者為最後幾個任務啟動第二份副本。
  * 若工作節點因硬體或軟體錯誤計算出錯誤輸出？
    . 無解！MapReduce 假設 CPU 和軟體為「故障停止」（fail-stop）。
  * 若協調者當機？
    .（未明確說明，可能需手動重啟或設計其他恢復機制。）

Performance?
  Figure 2
  X-Axis is time
  Y-Axis is total rate at which a "grep"-style job reads its input
  A terabyte (1000 GB) of input
  1764 workers
  30,000 MB/s (30 GB/s) is huge!
  Why 30,000 MB/s?
    17 MB/s per worker machine -- 140 megabits/second
    more than our guess (55 mbit/s) of net bandwidth
    input probably read direct from local GFS disk
    so disks probably could read at about 17 MB/second
  Why is the main period of activity about 30 seconds?
  Why does it take 50 seconds for throughput to reach maximum?
效能表現
  圖 2：
  X 軸：時間。
  Y 軸：類似 "grep" 的作業讀取輸入的總速率。
  輸入數據：1 TB（1000 GB）。
  1764 個工作節點。
  總吞吐量 30,000 MB/s（30 GB/s），非常驚人！
  為什麼達到 30,000 MB/s？
    . 每台工作節點約 17 MB/s（140 Mbit/s）。
    . 高於預估的網路頻寬（55 Mbit/s）。
    . 輸入數據可能直接從本地 GFS 磁碟讀取。
    . 磁碟讀取速度約 17 MB/s。
  為什麼主要活動週期約 30 秒？
  為什麼吞吐量需 50 秒才達最大？

Current status?
  Hugely influential (Hadoop, Spark, &c).
  Probably no longer in use at Google.
    Replaced by Flume / FlumeJava (see paper by Chambers et al).
    GFS replaced by Colossus (no good description), and BigTable.
Conclusion
  MapReduce made big cluster computation popular.
  - Not the most efficient or flexible.
  + Scales well.
  + Easy to program -- MR hides failures and data movement.
  These were good trade-offs in practice.
  We'll see some more advanced successors later in the course.
  Have fun with Lab 1!
當前狀態
  . MapReduce 極具影響力（啟發 Hadoop、Spark 等）。
  . Google 可能已不再使用 MapReduce：
    - 改用 Flume / FlumeJava（參考 Chambers 等人的論文）。
    - GFS 已被 Colossus 取代（無詳細公開描述），以及 BigTable。
結論
  . MapReduce 讓大型叢集計算普及化：
    - 缺點：不夠高效或靈活。
    - 優點：可擴展性強、易於程式設計——隱藏故障和數據移動。
  . 這些在實務中是很好的權衡。
  . 課程後續將介紹更先進的後續技術。
  . 祝實驗 1 愉快！
